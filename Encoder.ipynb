{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, d_k, d_model, n_heads):\n",
    "    super().__init__()\n",
    "\n",
    "    # Assume d_v = d_k\n",
    "    self.d_k = d_k\n",
    "    self.n_heads = n_heads\n",
    "\n",
    "    self.key = nn.Linear(d_model, d_k * n_heads)\n",
    "    self.query = nn.Linear(d_model, d_k * n_heads)\n",
    "    self.value = nn.Linear(d_model, d_k * n_heads)\n",
    "\n",
    "    # final linear layer\n",
    "    self.fc = nn.Linear(d_k * n_heads, d_model)\n",
    "\n",
    "  def forward(self, q, k, v, mask=None):\n",
    "    q = self.query(q) # N x T x (hd_k)\n",
    "    k = self.key(k)   # N x T x (hd_k)\n",
    "    v = self.value(v) # N x T x (hd_v)\n",
    "\n",
    "    N = q.shape[0]\n",
    "    T = q.shape[1]\n",
    "\n",
    "    # change the shape to:\n",
    "    # (N, T, h, d_k) -> (N, h, T, d_k)\n",
    "    # in order for matrix multiply to work properly\n",
    "    q = q.view(N, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "    k = k.view(N, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "    v = v.view(N, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "    # compute attention weights\n",
    "    # (N, h, T, d_k) x (N, h, d_k, T) --> (N, h, T, T)\n",
    "    attn_scores = q @ k.transpose(-2, -1) / math.sqrt(self.d_k)\n",
    "    if mask is not None:\n",
    "      attn_scores = attn_scores.masked_fill(\n",
    "          mask[:, None, None, :] == 0, float('-inf'))\n",
    "    attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "    \n",
    "    # compute attention-weighted values\n",
    "    # (N, h, T, T) x (N, h, T, d_k) --> (N, h, T, d_k)\n",
    "    A = attn_weights @ v\n",
    "\n",
    "    # reshape it back before final linear layer\n",
    "    A = A.transpose(1, 2) # (N, T, h, d_k)\n",
    "    A = A.contiguous().view(N, T, self.d_k * self.n_heads) # (N, T, h*d_k)\n",
    "\n",
    "    # projection\n",
    "    return self.fc(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "  def __init__(self, d_k, d_model, n_heads, dropout_prob=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.ln1 = nn.LayerNorm(d_model)\n",
    "    self.ln2 = nn.LayerNorm(d_model)\n",
    "    self.mha = MultiHeadAttention(d_k, d_model, n_heads)\n",
    "    self.ann = nn.Sequential(\n",
    "        nn.Linear(d_model, d_model * 4),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(d_model * 4, d_model),\n",
    "        nn.Dropout(dropout_prob),\n",
    "    )\n",
    "    self.dropout = nn.Dropout(p=dropout_prob)\n",
    "  \n",
    "  def forward(self, x, mask=None):\n",
    "    x = self.ln1(x + self.mha(x, x, x, mask))\n",
    "    x = self.ln2(x + self.ann(x))\n",
    "    x = self.dropout(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "  def __init__(self, d_model, max_len=2048, dropout_prob=0.1):\n",
    "    super().__init__()\n",
    "    self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "    position = torch.arange(max_len).unsqueeze(1)\n",
    "    exp_term = torch.arange(0, d_model, 2)\n",
    "    div_term = torch.exp(exp_term * (-math.log(10000.0) / d_model))\n",
    "    pe = torch.zeros(1, max_len, d_model)\n",
    "    pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "    pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "    self.register_buffer('pe', pe)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # x.shape: N x T x D\n",
    "    x = x + self.pe[:, :x.size(1), :]\n",
    "    return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self,\n",
    "               vocab_size,\n",
    "               max_len,\n",
    "               d_k,\n",
    "               d_model,\n",
    "               n_heads,\n",
    "               n_layers,\n",
    "               n_classes,\n",
    "               dropout_prob):\n",
    "    super().__init__()\n",
    "\n",
    "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "    self.pos_encoding = PositionalEncoding(d_model, max_len, dropout_prob)\n",
    "    transformer_blocks = [\n",
    "        TransformerBlock(\n",
    "            d_k,\n",
    "            d_model,\n",
    "            n_heads,\n",
    "            dropout_prob) for _ in range(n_layers)]\n",
    "    self.transformer_blocks = nn.Sequential(*transformer_blocks)\n",
    "    self.ln = nn.LayerNorm(d_model)\n",
    "    self.fc = nn.Linear(d_model, n_classes)\n",
    "  \n",
    "  def forward(self, x, mask=None):\n",
    "    x = self.embedding(x)\n",
    "    x = self.pos_encoding(x)\n",
    "    for block in self.transformer_blocks:\n",
    "      x = block(x, mask)\n",
    "\n",
    "    # many-to-one (x has the shape N x T x D)\n",
    "    x = x[:, 0, :]\n",
    "\n",
    "    x = self.ln(x)\n",
    "    x = self.fc(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Encoder(20_000,1024,16,64,4,2,5,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (embedding): Embedding(20000, 64)\n",
       "  (pos_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): MultiHeadAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (ann): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): MultiHeadAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (ann): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc): Linear(in_features=64, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randint(0, 20_000, (8,512))\n",
    "x_t = torch.tensor(x).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.ones((8, 512))\n",
    "mask[:, 256:] = 0\n",
    "mask_t = torch.tensor(mask).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model(x_t, mask_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 5])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chent\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chent\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'distilbert-base-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 35.3k/35.3k [00:00<?, ?B/s]\n",
      "Downloading data: 100%|██████████| 3.11M/3.11M [00:01<00:00, 2.68MB/s]\n",
      "Downloading data: 100%|██████████| 72.8k/72.8k [00:00<00:00, 203kB/s]\n",
      "Downloading data: 100%|██████████| 148k/148k [00:00<00:00, 268kB/s]\n",
      "Generating train split: 100%|██████████| 67349/67349 [00:00<00:00, 2104636.30 examples/s]\n",
      "Generating validation split: 100%|██████████| 872/872 [00:00<00:00, 435979.63 examples/s]\n",
      "Generating test split: 100%|██████████| 1821/1821 [00:00<00:00, 1821566.32 examples/s]\n"
     ]
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"glue\", \"sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'that loves its characters and communicates something rather beautiful about human nature ',\n",
       " 'label': 1,\n",
       " 'idx': 2}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(batch):\n",
    "  return tokenizer(batch['sentence'], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 67349/67349 [00:01<00:00, 48768.32 examples/s]\n",
      "Map: 100%|██████████| 872/872 [00:00<00:00, 25646.04 examples/s]\n",
      "Map: 100%|██████████| 1821/1821 [00:00<00:00, 33108.19 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(tokenize_fn, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorWithPadding(tokenizer=DistilBertTokenizerFast(name_or_path='distilbert-base-cased', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}, padding=True, max_length=None, pad_to_multiple_of=None, return_tensors='pt')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence\", \"idx\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=32,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"],\n",
    "    batch_size=32,\n",
    "    collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(tokenized_datasets['train']['labels'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28996"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (embedding): Embedding(28996, 64)\n",
       "  (pos_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): MultiHeadAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (ann): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): MultiHeadAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (ann): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Encoder(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_len=tokenizer.model_max_length,\n",
    "    d_k=16,\n",
    "    d_model=64,\n",
    "    n_heads=4,\n",
    "    n_layers=2,\n",
    "    n_classes=2,\n",
    "    dropout_prob=0.1,\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to encapsulate the training loop\n",
    "def train(model, criterion, optimizer, train_loader, valid_loader, epochs):\n",
    "  train_losses = np.zeros(epochs)\n",
    "  test_losses = np.zeros(epochs)\n",
    "\n",
    "  for it in range(epochs):\n",
    "    model.train()\n",
    "    t0 = datetime.now()\n",
    "    train_loss = 0\n",
    "    n_train = 0\n",
    "    for batch in train_loader:\n",
    "      # move data to GPU\n",
    "      batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "      # zero the parameter gradients\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # Forward pass\n",
    "      outputs = model(batch['input_ids'], batch['attention_mask'])\n",
    "      loss = criterion(outputs, batch['labels'])\n",
    "        \n",
    "      # Backward and optimize\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      train_loss += loss.item()*batch['input_ids'].size(0)\n",
    "      n_train += batch['input_ids'].size(0)\n",
    "\n",
    "    # Get average train loss\n",
    "    train_loss = train_loss / n_train\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    n_test = 0\n",
    "    for batch in valid_loader:\n",
    "      batch = {k: v.to(device) for k, v in batch.items()}\n",
    "      outputs = model(batch['input_ids'], batch['attention_mask'])\n",
    "      loss = criterion(outputs, batch['labels'])\n",
    "      test_loss += loss.item()*batch['input_ids'].size(0)\n",
    "      n_test += batch['input_ids'].size(0)\n",
    "    test_loss = test_loss / n_test\n",
    "\n",
    "    # Save losses\n",
    "    train_losses[it] = train_loss\n",
    "    test_losses[it] = test_loss\n",
    "    \n",
    "    dt = datetime.now() - t0\n",
    "    print(f'Epoch {it+1}/{epochs}, Train Loss: {train_loss:.4f}, \\\n",
    "      Test Loss: {test_loss:.4f}, Duration: {dt}')\n",
    "  \n",
    "  return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4, Train Loss: 0.5306,       Test Loss: 0.4822, Duration: 0:00:20.374830\n",
      "Epoch 2/4, Train Loss: 0.3629,       Test Loss: 0.4594, Duration: 0:00:20.976000\n",
      "Epoch 3/4, Train Loss: 0.2939,       Test Loss: 0.5810, Duration: 0:00:20.380999\n",
      "Epoch 4/4, Train Loss: 0.2522,       Test Loss: 0.4955, Duration: 0:00:20.748999\n"
     ]
    }
   ],
   "source": [
    "train_losses, test_losses = train(\n",
    "    model, criterion, optimizer, train_loader, valid_loader, epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.9350, Test acc: 0.8016\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "\n",
    "model.eval()\n",
    "n_correct = 0.\n",
    "n_total = 0.\n",
    "for batch in train_loader:\n",
    "  # Move to GPU\n",
    "  batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "  # Forward pass\n",
    "  outputs = model(batch['input_ids'], batch['attention_mask'])\n",
    "\n",
    "  # Get prediction\n",
    "  # torch.max returns both max and argmax\n",
    "  _, predictions = torch.max(outputs, 1)\n",
    "  \n",
    "  # update counts\n",
    "  n_correct += (predictions == batch['labels']).sum().item()\n",
    "  n_total += batch['labels'].shape[0]\n",
    "\n",
    "train_acc = n_correct / n_total\n",
    "\n",
    "\n",
    "n_correct = 0.\n",
    "n_total = 0.\n",
    "for batch in valid_loader:\n",
    "  # Move to GPU\n",
    "  batch = {k: v.to(device) for k, v in batch.items()}\n",
    "  \n",
    "  # Forward pass\n",
    "  outputs = model(batch['input_ids'], batch['attention_mask'])\n",
    "\n",
    "  # Get prediction\n",
    "  # torch.max returns both max and argmax\n",
    "  _, predictions = torch.max(outputs, 1)\n",
    "  \n",
    "  # update counts\n",
    "  n_correct += (predictions == batch['labels']).sum().item()\n",
    "  n_total += batch['labels'].shape[0]\n",
    "\n",
    "test_acc = n_correct / n_total\n",
    "print(f\"Train acc: {train_acc:.4f}, Test acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
